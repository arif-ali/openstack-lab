heat_template_version: 2014-10-16

description: >
  This template will boot a Kubernetes cluster with one or more
  minions (as specified by the number_of_minions parameter, which
  defaults to "2").

parameters:

  #
  # REQUIRED PARAMETERS
  #
  key_name:
    type: string
    description: name of ssh key to be provisioned on our server
  public_net_id:
    type: string
    description: uuid of a network to use for floating ip addresses
  #private_net_id:
  #  type: string
  #  description: uuid of a network to use for floating ip addresses
  #private_subnet_id:
  #  type: string
  #  description: uuid of a network to use for floating ip addresses

  #
  # OPTIONAL PARAMETERS
  #
  image:
    type: string
    default: centos7
    description: glance image used to boot the server

  flavor:
    type: string
    default: m1.small
    description: flavor to use when booting the server

  dns_nameserver:
    type: string
    description: address of a dns nameserver reachable in your environment
    default: 8.8.8.8

  number_of_compute_nodes:
    type: string
    description: how many compute nodes to spawn
    default: 10

resources:

  master_wait_handle:
    type: "AWS::CloudFormation::WaitConditionHandle"

  master_wait_condition:
    type: "AWS::CloudFormation::WaitCondition"
    depends_on:
      - master_node
    properties:
      Handle:
        get_resource: master_wait_handle
      Timeout: "300"

  ######################################################################
  #
  # network resources.  allocate a network and router for our server.
  # it would also be possible to take advantage of existing network
  # resources (and have the deployer provide network and subnet ids,
  # etc, as parameters), but I wanted to minmize the amount of
  # configuration necessary to make this go.
  fixed_net:
    type: "OS::Neutron::Net"

  # This is the subnet on which we will deploy our server.
  fixed_subnet:
    type: "OS::Neutron::Subnet"
    properties:
      cidr: 10.0.9.0/24
      network_id: { get_resource: fixed_net }
      dns_nameservers: [{ get_param: dns_nameserver }]

  # create a router attached to the external network provided as a
  # parameter to this stack.
  extrouter:
    type: "OS::Neutron::Router"
    properties:
      external_gateway_info:
        network:
          get_param: public_net_id

  # attached fixed_subnet to our extrouter router.
  extrouter_inside:
    type: "OS::Neutron::RouterInterface"
    properties:
      router_id:
        get_resource: extrouter
      subnet_id: { get_resource: fixed_subnet }

  ######################################################################
  #
  # security groups.  we need to permit network traffic of various
  # sorts.
  #

  secgroup_base:
    type: "OS::Neutron::SecurityGroup"
    properties:
      rules:
        - protocol: icmp
        - protocol: tcp
          port_range_min: 22
          port_range_max: 22

  secgroup_compute:
    type: "OS::Neutron::SecurityGroup"
    properties:
      rules:
        - protocol: tcp
          port_range_min: 22
          port_range_max: 22
        - protocol: tcp
          port_range_min: 4001
          port_range_max: 4001

  ######################################################################
  #
  # databases server.  this sets up a MySQL server
  #
  master_node:
    type: "OS::Nova::Server"
    depends_on:
      - extrouter_inside
    properties:
      name: master
      image:
        get_param: image
      flavor:
        get_param: flavor
      key_name:
        get_param: key_name
      user_data_format: RAW
      user_data:
        str_replace:
          template: |
            #!/bin/sh

            yum -y upgrade

            cat > /etc/yum.repos.d/slurm.repo << EOF
            [slurm]
            name=slurm
            baseurl=http://192.168.33.200/install/post/otherpkgs/el7/slurm
            enabled=1
            gpgcheck=0
            EOF

            yum -y install slurm slurm-slurmdbd slurm-munge

            dd if=/dev/urandom bs=1 count=1024 /etc/munge.key
            chmod 400 /etc/munge.key

            service munge restart


            cat > /tmp/wait-data << EOF
            {
              "Status" : "SUCCESS",
              "Reason" : "Setup Complete",
              "UniqueId" : "None",
              "Data" : "OK"
            }
            EOF
            curl -T /tmp/wait-data '$WAIT_HANDLE'
          params:
            "$COMP_NODE_ADDRESSES": {list_join: [",", {get_attr: [compute_nodes, compute_node_ip]}]}
            "$WAIT_HANDLE":
              get_resource: master_wait_handle
      networks:
        - port:
            get_resource: master_node_eth0

  master_node_eth0:
    type: "OS::Neutron::Port"
    properties:
      network_id:
        get_resource: fixed_net
      security_groups:
        - get_resource: secgroup_base
        - get_resource: secgroup_compute
      fixed_ips:
        - subnet_id:
            get_resource: fixed_subnet

  master_node_floating:
    type: "OS::Neutron::FloatingIP"
    depends_on:
      - extrouter_inside
    properties:
      floating_network_id:
        get_param: public_net_id
      port_id:
        get_resource: master_node_eth0

  compute_nodes:
    type: "OS::Heat::ResourceGroup"
    depends_on:
      - extrouter_inside
    properties:
      count: {get_param: number_of_compute_nodes}
      resource_def:
        type: compute_node.yaml
        properties:
          index_name: "%index%"
          key_name: {get_param: key_name}
          image: {get_param: image}
          flavor: {get_param: flavor}
          private_net_id: {get_resource: fixed_net}
          private_subnet_id: {get_resource: fixed_subnet}
          public_net_id: {get_param: public_net_id}
          master_node_ip: {get_attr: [master_node_eth0, fixed_ips, 0, ip_address]}

outputs:

  master_node:
    value: {get_attr: [master_node_floating, floating_ip_address]}

  compute_nodes:
    value: {get_attr: [compute_nodes, compute_node_ip]}

  compute_node_external:
    value: {get_attr: [compute_nodes, compute_node_external_ip]}
